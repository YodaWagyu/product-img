import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import random
import re
from datetime import datetime

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏´‡∏ô‡πâ‡∏≤ Web App
st.set_page_config(page_title="Product Image Scraper", layout="wide")

st.title("ÔøΩ Product Image Scraper")
st.markdown("""
‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏à‡∏≤‡∏Å‡πÄ‡∏ß‡πá‡∏ö E-commerce ‡πÇ‡∏î‡∏¢‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡∏•‡∏∞‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏ô‡∏Ñ‡∏£‡∏ö ‡∏´‡∏£‡∏∑‡∏≠‡∏à‡∏ô‡∏Å‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏ñ‡∏∂‡∏á‡∏Ç‡∏µ‡∏î‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î
""")

# --- ‡∏™‡πà‡∏ß‡∏ô‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ (Configuration) ---
with st.expander("‚öôÔ∏è ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Settings)", expanded=True):
    col1, col2 = st.columns([2, 1])
    with col1:
        base_category_url = st.text_input(
            "üîó URL ‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤",
            value="",
            placeholder="‡∏ß‡∏≤‡∏á URL ‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£...",
            help="‡πÉ‡∏™‡πà URL ‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"
        )
    
    with col2:
        max_pages = st.number_input(
            "‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î (Max Pages)", 
            min_value=1, 
            max_value=100, 
            value=5,
            help="‡πÉ‡∏™‡πà‡πÄ‡∏•‡∏Ç‡πÄ‡∏¢‡∏≠‡∏∞‡πÜ ‡πÄ‡∏ä‡πà‡∏ô 100 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏∂‡∏á‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö‡∏ó‡∏∏‡∏Å‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏µ"
        )

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Loop ‡∏´‡∏•‡∏≤‡∏¢‡∏´‡∏ô‡πâ‡∏≤)
def scrape_all_pages(base_url, max_pages):
    all_data = []
    
    status_text = st.empty()
    progress_bar = st.progress(0)
    
    current_timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á Session ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏Å‡πá‡∏ö cookies ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
    session = requests.Session()
    
    # Headers ‡πÄ‡∏ï‡πá‡∏°‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô browser ‡∏à‡∏£‡∏¥‡∏á
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
        'Accept-Language': 'th-TH,th;q=0.9,en-US;q=0.8,en;q=0.7',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'Sec-Fetch-Dest': 'document',
        'Sec-Fetch-Mode': 'navigate',
        'Sec-Fetch-Site': 'none',
        'Sec-Fetch-User': '?1',
        'Cache-Control': 'max-age=0',
    }
    session.headers.update(headers)
    
    # ‡πÄ‡∏Ç‡πâ‡∏≤‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏£‡∏Å‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏±‡∏ö cookies
    try:
        session.get(base_url, timeout=10)
        time.sleep(1)
    except:
        pass

    for page in range(1, max_pages + 1):
        current_url = f"{base_url}?limit=100&page={page}"
        
        status_text.text(f"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•... ‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà {page}/{max_pages}")
        progress_bar.progress(page / max_pages)

        try:
            response = session.get(current_url, timeout=15)
            
            if response.status_code != 200:
                st.warning(f"‡∏´‡∏ô‡πâ‡∏≤ {page} ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à (Status: {response.status_code}) ‡∏Ç‡πâ‡∏≤‡∏°‡πÑ‡∏õ‡∏´‡∏ô‡πâ‡∏≤‡∏ñ‡∏±‡∏î‡πÑ‡∏õ...")
                continue
                
            soup = BeautifulSoup(response.content, 'html.parser')
            
            product_cards = soup.find_all('div', class_=re.compile(r'productCard_container_'))
            
            if not product_cards:
                status_text.success(f"‡∏™‡∏¥‡πâ‡∏ô‡∏™‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏´‡∏ô‡πâ‡∏≤ {page-1}")
                break

            for card in product_cards:
                item = {}
                
                item['Scraped Date'] = current_timestamp

                name_el = card.find(class_=re.compile(r'productCard_title_'))
                item['Product Name'] = name_el.get_text(strip=True) if name_el else "N/A"
                
                img_el = card.find('img')
                if img_el:
                    img_url = img_el.get('src') or img_el.get('data-src')
                    item['Image URL'] = img_url
                    
                    if img_url:
                        barcode_match = re.search(r'(\d{8,14})', img_url)
                        item['Barcode'] = barcode_match.group(1) if barcode_match else ""
                    else:
                        item['Barcode'] = ""
                else:
                    item['Image URL'] = ""
                    item['Barcode'] = ""

                price_container = card.find(class_=re.compile(r'productCard_price_'))
                if price_container:
                    prices_text = price_container.get_text(strip=True)
                    numbers = re.findall(r'[\d,]+', prices_text)
                    
                    if len(numbers) >= 2:
                        item['Promotion Price'] = numbers[0]
                        item['Normal Price'] = numbers[1]
                    elif len(numbers) == 1:
                        item['Normal Price'] = numbers[0]
                        item['Promotion Price'] = numbers[0]
                    else:
                        item['Normal Price'] = "N/A"
                        item['Promotion Price'] = "N/A"
                else:
                    item['Normal Price'] = "N/A"
                    item['Promotion Price'] = "N/A"

                all_data.append(item)
            
            time.sleep(random.uniform(0.5, 1.5))

        except Exception as e:
            st.error(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏ó‡∏µ‡πà‡∏´‡∏ô‡πâ‡∏≤ {page}: {e}")
            continue
            
    return pd.DataFrame(all_data)

# ‡∏õ‡∏∏‡πà‡∏° One-Click
if st.button("üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Start Scraping)", type="primary"):
    if not base_category_url:
        st.error("‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÉ‡∏™‡πà URL ‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•")
    else:
        df = scrape_all_pages(base_category_url, max_pages)
        
        if not df.empty:
            st.success(f"‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô! ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏°‡∏≤‡πÑ‡∏î‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(df)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
            
            st.dataframe(
                df,
                column_config={
                    "Image URL": st.column_config.ImageColumn("Image"),
                },
                use_container_width=True
            )
            
            csv = df.to_csv(index=False).encode('utf-8-sig')
            filename = f'products_{datetime.now().strftime("%Y%m%d_%H%M")}.csv'
            
            st.download_button(
                label=f"üíæ ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• {len(df)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ (CSV)",
                data=csv,
                file_name=filename,
                mime='text/csv',
            )
        else:
            st.warning("‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤ ‡∏≠‡∏≤‡∏à‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏ó‡∏µ‡πà‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö")
