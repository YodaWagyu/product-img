import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import random
import re
from datetime import datetime

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏´‡∏ô‡πâ‡∏≤ Web App
st.set_page_config(page_title="BigC Beauty Scraper", layout="wide")

st.title("üíÑ BigC Beauty & Personal Care Scraper")
st.markdown("""
‡πÅ‡∏≠‡∏õ‡∏û‡∏•‡∏¥‡πÄ‡∏Ñ‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏´‡∏°‡∏ß‡∏î **Beauty & Personal Care** ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÇ‡∏î‡∏¢‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡∏•‡∏∞‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏ô‡∏Ñ‡∏£‡∏ö ‡∏´‡∏£‡∏∑‡∏≠‡∏à‡∏ô‡∏Å‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏ñ‡∏∂‡∏á‡∏Ç‡∏µ‡∏î‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î
""")

# --- ‡∏™‡πà‡∏ß‡∏ô‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ (Configuration) ---
with st.expander("‚öôÔ∏è ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Settings)", expanded=True):
    col1, col2 = st.columns([2, 1])
    with col1:
        st.info("‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢: https://www.bigc.co.th/category/beauty-personal-care")
        base_category_url = "https://www.bigc.co.th/category/beauty-personal-care"
    
    with col2:
        # ‡πÉ‡∏´‡πâ User ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏î‡πâ‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏î‡∏∂‡∏á‡∏Å‡∏µ‡πà‡∏´‡∏ô‡πâ‡∏≤ (‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡πÅ‡∏Ñ‡πà‡∏≠‡∏¢‡∏≤‡∏Å‡πÄ‡∏ó‡∏™)
        max_pages = st.number_input(
            "‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á (Max Pages)", 
            min_value=1, 
            max_value=100, 
            value=5, # ‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÄ‡∏ã‡πá‡∏ï‡πÑ‡∏ß‡πâ 5 ‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÄ‡∏£‡πá‡∏ß‡πÜ ‡∏ñ‡πâ‡∏≤‡∏à‡∏∞‡πÄ‡∏≠‡∏≤‡∏´‡∏°‡∏î‡πÉ‡∏´‡πâ‡πÉ‡∏™‡πà‡πÄ‡∏¢‡∏≠‡∏∞‡πÜ
            help="‡πÉ‡∏™‡πà‡πÄ‡∏•‡∏Ç‡πÄ‡∏¢‡∏≠‡∏∞‡πÜ ‡πÄ‡∏ä‡πà‡∏ô 100 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏∂‡∏á‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö‡∏ó‡∏∏‡∏Å‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏µ"
        )

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Loop ‡∏´‡∏•‡∏≤‡∏¢‡∏´‡∏ô‡πâ‡∏≤)
def scrape_all_pages(base_url, max_pages):
    all_data = []
    
    # Placeholder ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÅ‡∏ö‡∏ö Real-time
    status_text = st.empty()
    progress_bar = st.progress(0)
    
    # ‡∏à‡∏±‡∏ö‡πÄ‡∏ß‡∏•‡∏≤‡∏ï‡∏≠‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    current_timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Accept-Language': 'th-TH,th;q=0.9,en;q=0.8'
    }

    # Loop ‡∏ï‡∏≤‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î
    for page in range(1, max_pages + 1):
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á URL ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏´‡∏ô‡πâ‡∏≤ (query string: limit=100&page=X)
        current_url = f"{base_url}?limit=100&page={page}"
        
        status_text.text(f"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•... ‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà {page}/{max_pages} ({current_url})")
        progress_bar.progress(page / max_pages)

        try:
            response = requests.get(current_url, headers=headers, timeout=10)
            
            if response.status_code != 200:
                st.warning(f"‡∏´‡∏ô‡πâ‡∏≤ {page} ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à (Status: {response.status_code}) ‡∏Ç‡πâ‡∏≤‡∏°‡πÑ‡∏õ‡∏´‡∏ô‡πâ‡∏≤‡∏ñ‡∏±‡∏î‡πÑ‡∏õ...")
                continue
                
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ Container ‡∏Ç‡∏≠‡∏á‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤
            product_cards = soup.find_all('div', class_=re.compile(r'productCard_container_'))
            
            # ‡∏ñ‡πâ‡∏≤‡∏´‡∏ô‡πâ‡∏≤‡∏ô‡∏µ‡πâ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡πÄ‡∏•‡∏¢ ‡πÅ‡∏™‡∏î‡∏á‡∏ß‡πà‡∏≤‡∏´‡∏°‡∏î‡πÅ‡∏•‡πâ‡∏ß -> ‡∏´‡∏¢‡∏∏‡∏î Loop ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ
            if not product_cards:
                status_text.success(f"‡∏™‡∏¥‡πâ‡∏ô‡∏™‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏´‡∏ô‡πâ‡∏≤ {page-1}")
                break

            # ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡πÉ‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ô‡∏±‡πâ‡∏ô‡πÜ
            for card in product_cards:
                item = {}
                
                # 0. Scraped Date (‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•)
                item['Scraped Date'] = current_timestamp

                # 1. Product Name
                name_el = card.find(class_=re.compile(r'productCard_title_'))
                item['Product Name'] = name_el.get_text(strip=True) if name_el else "N/A"
                
                # 2. Images & Barcode Extraction
                img_el = card.find('img')
                if img_el:
                    img_url = img_el.get('src') or img_el.get('data-src')
                    item['Image URL'] = img_url
                    
                    if img_url:
                        # ‡∏´‡∏≤‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç 8-14 ‡∏´‡∏•‡∏±‡∏Å‡πÉ‡∏ô URL
                        barcode_match = re.search(r'(\d{8,14})', img_url)
                        item['Barcode'] = barcode_match.group(1) if barcode_match else ""
                    else:
                        item['Barcode'] = ""
                else:
                    item['Image URL'] = ""
                    item['Barcode'] = ""

                # 3. Prices
                price_container = card.find(class_=re.compile(r'productCard_price_'))
                if price_container:
                    prices_text = price_container.get_text(strip=True)
                    numbers = re.findall(r'[\d,]+', prices_text)
                    
                    if len(numbers) >= 2:
                        item['Promotion Price'] = numbers[0]
                        item['Normal Price'] = numbers[1]
                    elif len(numbers) == 1:
                        item['Normal Price'] = numbers[0]
                        item['Promotion Price'] = numbers[0]
                    else:
                        item['Normal Price'] = "N/A"
                        item['Promotion Price'] = "N/A"
                else:
                    item['Normal Price'] = "N/A"
                    item['Promotion Price'] = "N/A"

                all_data.append(item)
            
            # ‡∏´‡∏ô‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡∏™‡∏∏‡πà‡∏° 1-2 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ ‡∏Å‡πà‡∏≠‡∏ô‡πÑ‡∏õ‡∏´‡∏ô‡πâ‡∏≤‡∏ñ‡∏±‡∏î‡πÑ‡∏õ (‡∏Å‡∏±‡∏ô‡πÇ‡∏î‡∏ô‡∏ö‡∏•‡πá‡∏≠‡∏Å)
            time.sleep(random.uniform(0.5, 1.5))

        except Exception as e:
            st.error(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏ó‡∏µ‡πà‡∏´‡∏ô‡πâ‡∏≤ {page}: {e}")
            continue
            
    return pd.DataFrame(all_data)

# ‡∏õ‡∏∏‡πà‡∏° One-Click
if st.button("üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (Start Scraping)", type="primary"):
    
    df = scrape_all_pages(base_category_url, max_pages)
    
    if not df.empty:
        st.success(f"‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô! ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏°‡∏≤‡πÑ‡∏î‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(df)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
        
        # ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        st.dataframe(
            df,
            column_config={
                "Image URL": st.column_config.ImageColumn("Image"),
            },
            use_container_width=True
        )
        
        # ‡∏õ‡∏∏‡πà‡∏°‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î
        csv = df.to_csv(index=False).encode('utf-8-sig')
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡πâ‡∏°‡∏µ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏õ‡∏∞‡∏ó‡πâ‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÅ‡∏¢‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏á‡πà‡∏≤‡∏¢‡πÜ
        filename = f'bigc_beauty_products_{datetime.now().strftime("%Y%m%d_%H%M")}.csv'
        
        st.download_button(
            label=f"üíæ ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• {len(df)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ (CSV)",
            data=csv,
            file_name=filename,
            mime='text/csv',
        )
    else:
        st.warning("‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡πÄ‡∏•‡∏¢ ‡∏≠‡∏≤‡∏à‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏ó‡∏µ‡πà‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö")
