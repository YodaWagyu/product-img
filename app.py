import streamlit as st
import cloudscraper
from bs4 import BeautifulSoup
import pandas as pd
import time
import random
import re
from datetime import datetime

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏´‡∏ô‡πâ‡∏≤ Web App
st.set_page_config(page_title="Product Image Scraper", layout="wide")

st.title("üõí Product Image Scraper")
st.markdown("""
‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏à‡∏≤‡∏Å‡πÄ‡∏ß‡πá‡∏ö E-commerce ‡πÇ‡∏î‡∏¢‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡∏•‡∏∞‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏ô‡∏Ñ‡∏£‡∏ö ‡∏´‡∏£‡∏∑‡∏≠‡∏à‡∏ô‡∏Å‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏ñ‡∏∂‡∏á‡∏Ç‡∏µ‡∏î‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î
""")

# --- ‡∏™‡πà‡∏ß‡∏ô‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ (Configuration) ---
with st.expander("‚öôÔ∏è ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Settings)", expanded=True):
    col1, col2 = st.columns([2, 1])
    with col1:
        base_category_url = st.text_input(
            "üîó URL ‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤",
            value="",
            placeholder="‡∏ß‡∏≤‡∏á URL ‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£...",
            help="‡πÉ‡∏™‡πà URL ‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"
        )
    
    with col2:
        max_pages = st.number_input(
            "‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î (Max Pages)", 
            min_value=1, 
            max_value=100, 
            value=5,
            help="‡πÉ‡∏™‡πà‡πÄ‡∏•‡∏Ç‡πÄ‡∏¢‡∏≠‡∏∞‡πÜ ‡πÄ‡∏ä‡πà‡∏ô 100 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏∂‡∏á‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö‡∏ó‡∏∏‡∏Å‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏µ"
        )

def scrape_all_pages(base_url, max_pages):
    all_data = []
    
    status_text = st.empty()
    progress_bar = st.progress(0)
    
    current_timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    # ‡πÉ‡∏ä‡πâ cloudscraper ‡πÄ‡∏û‡∏∑‡πà‡∏≠ bypass Cloudflare
    scraper = cloudscraper.create_scraper(
        browser={
            'browser': 'chrome',
            'platform': 'windows',
            'desktop': True
        }
    )
    
    for page in range(1, max_pages + 1):
        current_url = f"{base_url}?limit=100&page={page}"
        
        status_text.text(f"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•... ‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà {page}/{max_pages}")
        progress_bar.progress(page / max_pages)
        
        try:
            response = scraper.get(current_url, timeout=20)
            
            if response.status_code != 200:
                st.warning(f"‡∏´‡∏ô‡πâ‡∏≤ {page} ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à (Status: {response.status_code})")
                continue
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            product_cards = soup.find_all('div', class_=re.compile(r'productCard_container_'))
            
            if not product_cards:
                status_text.success(f"‡∏™‡∏¥‡πâ‡∏ô‡∏™‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏´‡∏ô‡πâ‡∏≤ {page-1}")
                break
            
            for card in product_cards:
                item = {}
                
                item['Scraped Date'] = current_timestamp
                
                name_el = card.find(class_=re.compile(r'productCard_title_'))
                item['Product Name'] = name_el.get_text(strip=True) if name_el else "N/A"
                
                img_el = card.find('img')
                if img_el:
                    img_url = img_el.get('src') or img_el.get('data-src')
                    item['Image URL'] = img_url
                    
                    if img_url:
                        barcode_match = re.search(r'(\d{8,14})', img_url)
                        item['Barcode'] = barcode_match.group(1) if barcode_match else ""
                    else:
                        item['Barcode'] = ""
                else:
                    item['Image URL'] = ""
                    item['Barcode'] = ""
                
                price_container = card.find(class_=re.compile(r'productCard_price_'))
                if price_container:
                    prices_text = price_container.get_text(strip=True)
                    numbers = re.findall(r'[\d,]+', prices_text)
                    
                    if len(numbers) >= 2:
                        item['Promotion Price'] = numbers[0]
                        item['Normal Price'] = numbers[1]
                    elif len(numbers) == 1:
                        item['Normal Price'] = numbers[0]
                        item['Promotion Price'] = numbers[0]
                    else:
                        item['Normal Price'] = "N/A"
                        item['Promotion Price'] = "N/A"
                else:
                    item['Normal Price'] = "N/A"
                    item['Promotion Price'] = "N/A"
                
                all_data.append(item)
            
            time.sleep(random.uniform(1, 2))
            
        except Exception as e:
            st.error(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏ó‡∏µ‡πà‡∏´‡∏ô‡πâ‡∏≤ {page}: {e}")
            continue
    
    return pd.DataFrame(all_data)

# ‡∏õ‡∏∏‡πà‡∏° One-Click
if st.button("üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Start Scraping)", type="primary"):
    if not base_category_url:
        st.error("‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÉ‡∏™‡πà URL ‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•")
    else:
        df = scrape_all_pages(base_category_url, max_pages)
        
        if not df.empty:
            st.success(f"‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô! ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏°‡∏≤‡πÑ‡∏î‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(df)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
            
            st.dataframe(
                df,
                column_config={
                    "Image URL": st.column_config.ImageColumn("Image"),
                },
                use_container_width=True
            )
            
            csv = df.to_csv(index=False).encode('utf-8-sig')
            filename = f'products_{datetime.now().strftime("%Y%m%d_%H%M")}.csv'
            
            st.download_button(
                label=f"üíæ ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• {len(df)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ (CSV)",
                data=csv,
                file_name=filename,
                mime='text/csv',
            )
        else:
            st.warning("‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤ ‡∏≠‡∏≤‡∏à‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏ó‡∏µ‡πà‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö")
