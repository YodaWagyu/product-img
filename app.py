import streamlit as st
import pandas as pd
import time
import random
import re
from datetime import datetime
from playwright.sync_api import sync_playwright

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏´‡∏ô‡πâ‡∏≤ Web App
st.set_page_config(page_title="Product Image Scraper", layout="wide")

st.title("üõí Product Image Scraper")
st.markdown("""
‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏à‡∏≤‡∏Å‡πÄ‡∏ß‡πá‡∏ö E-commerce ‡πÇ‡∏î‡∏¢‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡πÉ‡∏ä‡πâ Browser ‡∏à‡∏£‡∏¥‡∏á‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á‡∏Å‡∏≤‡∏£‡∏ñ‡∏π‡∏Å‡∏ö‡∏•‡πá‡∏≠‡∏Å
""")

# --- ‡∏™‡πà‡∏ß‡∏ô‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ (Configuration) ---
with st.expander("‚öôÔ∏è ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Settings)", expanded=True):
    col1, col2 = st.columns([2, 1])
    with col1:
        base_category_url = st.text_input(
            "üîó URL ‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤",
            value="",
            placeholder="‡∏ß‡∏≤‡∏á URL ‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£...",
            help="‡πÉ‡∏™‡πà URL ‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"
        )
    
    with col2:
        max_pages = st.number_input(
            "‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î (Max Pages)", 
            min_value=1, 
            max_value=100, 
            value=5,
            help="‡πÉ‡∏™‡πà‡πÄ‡∏•‡∏Ç‡πÄ‡∏¢‡∏≠‡∏∞‡πÜ ‡πÄ‡∏ä‡πà‡∏ô 100 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏∂‡∏á‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö‡∏ó‡∏∏‡∏Å‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏µ"
        )

def scrape_with_browser(base_url, max_pages):
    all_data = []
    
    status_text = st.empty()
    progress_bar = st.progress(0)
    
    current_timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    status_text.text("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏õ‡∏¥‡∏î Browser...")
    
    with sync_playwright() as p:
        # ‡πÄ‡∏õ‡∏¥‡∏î Browser ‡πÅ‡∏ö‡∏ö headless
        browser = p.chromium.launch(headless=True)
        context = browser.new_context(
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            viewport={'width': 1920, 'height': 1080},
            locale='th-TH'
        )
        page = context.new_page()
        
        for page_num in range(1, max_pages + 1):
            current_url = f"{base_url}?limit=100&page={page_num}"
            
            status_text.text(f"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•... ‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà {page_num}/{max_pages}")
            progress_bar.progress(page_num / max_pages)
            
            try:
                # ‡πÄ‡∏Ç‡πâ‡∏≤‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö
                page.goto(current_url, wait_until='networkidle', timeout=30000)
                
                # ‡∏£‡∏≠‡πÉ‡∏´‡πâ product cards ‡πÇ‡∏´‡∏•‡∏î
                page.wait_for_selector('[class*="productCard_container_"]', timeout=10000)
                
                # ‡∏î‡∏∂‡∏á HTML
                content = page.content()
                
                from bs4 import BeautifulSoup
                soup = BeautifulSoup(content, 'html.parser')
                
                product_cards = soup.find_all('div', class_=re.compile(r'productCard_container_'))
                
                if not product_cards:
                    status_text.success(f"‡∏™‡∏¥‡πâ‡∏ô‡∏™‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏´‡∏ô‡πâ‡∏≤ {page_num-1}")
                    break
                
                for card in product_cards:
                    item = {}
                    
                    item['Scraped Date'] = current_timestamp
                    
                    name_el = card.find(class_=re.compile(r'productCard_title_'))
                    item['Product Name'] = name_el.get_text(strip=True) if name_el else "N/A"
                    
                    img_el = card.find('img')
                    if img_el:
                        img_url = img_el.get('src') or img_el.get('data-src')
                        item['Image URL'] = img_url
                        
                        if img_url:
                            barcode_match = re.search(r'(\d{8,14})', img_url)
                            item['Barcode'] = barcode_match.group(1) if barcode_match else ""
                        else:
                            item['Barcode'] = ""
                    else:
                        item['Image URL'] = ""
                        item['Barcode'] = ""
                    
                    price_container = card.find(class_=re.compile(r'productCard_price_'))
                    if price_container:
                        prices_text = price_container.get_text(strip=True)
                        numbers = re.findall(r'[\d,]+', prices_text)
                        
                        if len(numbers) >= 2:
                            item['Promotion Price'] = numbers[0]
                            item['Normal Price'] = numbers[1]
                        elif len(numbers) == 1:
                            item['Normal Price'] = numbers[0]
                            item['Promotion Price'] = numbers[0]
                        else:
                            item['Normal Price'] = "N/A"
                            item['Promotion Price'] = "N/A"
                    else:
                        item['Normal Price'] = "N/A"
                        item['Promotion Price'] = "N/A"
                    
                    all_data.append(item)
                
                # ‡∏´‡∏ô‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡∏™‡∏∏‡πà‡∏°
                time.sleep(random.uniform(1, 2))
                
            except Exception as e:
                st.warning(f"‡∏´‡∏ô‡πâ‡∏≤ {page_num}: {str(e)[:100]}")
                continue
        
        browser.close()
    
    return pd.DataFrame(all_data)

# ‡∏õ‡∏∏‡πà‡∏° One-Click
if st.button("üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Start Scraping)", type="primary"):
    if not base_category_url:
        st.error("‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÉ‡∏™‡πà URL ‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•")
    else:
        try:
            df = scrape_with_browser(base_category_url, max_pages)
            
            if not df.empty:
                st.success(f"‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô! ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏°‡∏≤‡πÑ‡∏î‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(df)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
                
                st.dataframe(
                    df,
                    column_config={
                        "Image URL": st.column_config.ImageColumn("Image"),
                    },
                    use_container_width=True
                )
                
                csv = df.to_csv(index=False).encode('utf-8-sig')
                filename = f'products_{datetime.now().strftime("%Y%m%d_%H%M")}.csv'
                
                st.download_button(
                    label=f"üíæ ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• {len(df)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ (CSV)",
                    data=csv,
                    file_name=filename,
                    mime='text/csv',
                )
            else:
                st.warning("‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤ ‡∏≠‡∏≤‡∏à‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏ó‡∏µ‡πà‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö")
        except Exception as e:
            st.error(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}")
